{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "146a27c2-3aa6-4a39-b17f-32e6494007a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Facial-Similarity-with-Siamese-Networks-in-Pytorch'...\n",
      "remote: Enumerating objects: 550, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
      "remote: Total 550 (delta 6), reused 18 (delta 6), pack-reused 532\u001b[K\n",
      "Receiving objects: 100% (550/550), 6.32 MiB | 6.99 MiB/s, done.\n",
      "Resolving deltas: 100% (27/27), done.\n",
      "Updating files: 100% (405/405), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch.git\n",
    "!mkdir data\n",
    "!mv Facial-Similarity-with-Siamese-Networks-in-Pytorch/data/faces/testing/* data/\n",
    "!mv Facial-Similarity-with-Siamese-Networks-in-Pytorch/data/faces/training/* data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250f549-eb18-4b90-8414-aa100e559a40",
   "metadata": {},
   "source": [
    "In this tutorial, we will show how you can use AIJack to debug and improve the trained neural network with Neuron Coverage proposed in `Pei, Kexin, et al. \"Deepxplore: Automated whitebox testing of deep learning systems.\" proceedings of the 26th Symposium on Operating Systems Principles. 2017`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76512b3e-d35b-4e75-8b90-3356e77e5ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from aijack.utils import NumpyDataset\n",
    "from aijack.defense.debugging.neuroncoverage import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffd8120-06f5-4a24-add9-5992bb1da423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "fix_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c852ebe-2752-414a-87bf-cb6b5b47da40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fla = nn.Flatten()\n",
    "        self.fc = nn.Linear(112 * 92, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fla(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def split_dataloader(data_loader, k):\n",
    "    dataset = data_loader.dataset\n",
    "    dataset_size = len(dataset)\n",
    "    batch_size = data_loader.batch_size\n",
    "\n",
    "    # Calculate the size of each subset\n",
    "    subset_size = dataset_size // k\n",
    "    remainder = dataset_size % k\n",
    "\n",
    "    # Create a list to store the k DataLoaders\n",
    "    dataloaders = []\n",
    "\n",
    "    # Create subsets and DataLoaders\n",
    "    start_idx = 0\n",
    "    for i in range(k):\n",
    "        end_idx = start_idx + subset_size + (1 if i < remainder else 0)\n",
    "        indices = list(range(start_idx, end_idx))\n",
    "        sampler = SubsetRandomSampler(indices)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "        dataloaders.append(dataloader)\n",
    "        start_idx = end_idx\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89401d7-76a3-4756-8e05-4af539661237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE = \"data/\"\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "for i in range(1, 41):\n",
    "    for j in range(1, 11):\n",
    "        img = cv2.imread(BASE + f\"s{i}/{j}.pgm\", 0)\n",
    "        imgs.append(img)\n",
    "        labels.append(i - 1)\n",
    "\n",
    "X = np.stack(imgs)\n",
    "y = np.array(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# ToTensor：画像のグレースケール化（RGBの0~255を0~1の範囲に正規化）、Normalize：Z値化（RGBの平均と標準偏差を0.5で決め打ちして正規化）\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "trainset = NumpyDataset(X_train, y_train, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=4, shuffle=True, num_workers=2\n",
    ")\n",
    "testset = NumpyDataset(X_test, y_test, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=4, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa51e2b1-b968-41ea-a1f1-d2867810855c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss is 0.9179768215364484\n",
      "epoch 1: loss is 0.8781074712525553\n",
      "epoch 2: loss is 0.8334654267154523\n",
      "epoch 3: loss is 0.8095823072675449\n",
      "epoch 4: loss is 0.7987308706810226\n",
      "epoch 5: loss is 0.7798728791635428\n",
      "epoch 6: loss is 0.7634210195114364\n",
      "epoch 7: loss is 0.7491738876300071\n",
      "epoch 8: loss is 0.7392014594220403\n",
      "epoch 9: loss is 0.7327400935229971\n",
      "Test Accuracy is:  0.6742424242424242\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0\n",
    "    data_size = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.to(torch.int64))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        data_size += inputs.shape[0]\n",
    "\n",
    "    print(f\"epoch {epoch}: loss is {running_loss/data_size}\")\n",
    "\n",
    "\n",
    "in_preds = []\n",
    "in_label = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        in_preds.append(outputs)\n",
    "        in_label.append(labels)\n",
    "    in_preds = torch.cat(in_preds)\n",
    "    in_label = torch.cat(in_label)\n",
    "print(\n",
    "    \"Test Accuracy is: \",\n",
    "    accuracy_score(np.array(torch.argmax(in_preds, axis=1)), np.array(in_label)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83cc17-1116-468b-831b-06e40c14e47e",
   "metadata": {},
   "source": [
    "We then generate additional data that increases Neuron Coverage to improve the performance of the model based on the prior study `Yang, Zhou, et al. \"Revisiting neuron coverage metrics and quality of deep neural networks.\" 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2022`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57d823d-e9f5-4b32-bf1c-3a800eb637b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (before) is:  0.6742424242424242\n",
      "NC (before):  0.7749999761581421\n",
      "----\n",
      "15 test cases generated\n",
      "Test Accuracy (after) is:  0.8333333333333334\n",
      "NC (after):  0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "split_dataloaders = split_dataloader(trainloader, k)\n",
    "\n",
    "additional_x = []\n",
    "additional_y = []\n",
    "t = 0.5\n",
    "\n",
    "print(\n",
    "    \"Test Accuracy (before) is: \",\n",
    "    accuracy_score(np.array(torch.argmax(in_preds, axis=1)), np.array(in_label)),\n",
    ")\n",
    "NCT = neuroncoverage.NeuronCoverageTracker(\n",
    "    net, threshold=t, dummy_data=inputs[[0]], device=\"cpu\"\n",
    ")\n",
    "nc = NCT.coverage(trainloader)\n",
    "print(\"NC (before): \", nc)\n",
    "\n",
    "for k, sd in enumerate(split_dataloaders):\n",
    "    for db in sd:\n",
    "        xb, yb = db\n",
    "        for x, y in zip(xb, yb):\n",
    "            x += torch.randn(x.shape) * 0.03\n",
    "            x = x.reshape(-1, 1, 112, 92)\n",
    "            yp = net(x)\n",
    "            if yp.argmax().item() != y.item():\n",
    "                ncu = NCT.coverage([x], initialize=False, update=False)\n",
    "                if ncu > nc:\n",
    "                    additional_x.append(x.numpy()[0][0])\n",
    "                    additional_y.append(y.numpy().item())\n",
    "\n",
    "    if len(additional_x) == 0:\n",
    "        continue\n",
    "\n",
    "    X_train_augmented = np.concatenate(\n",
    "        [X_train, np.stack(additional_x).astype(np.uint8)]\n",
    "    )\n",
    "    y_train_augmented = np.concatenate([y_train, np.array(additional_y)])\n",
    "\n",
    "    trainset_augmented = NumpyDataset(\n",
    "        X_train_augmented, y_train_augmented, transform=transform\n",
    "    )\n",
    "    trainloader_augmented = torch.utils.data.DataLoader(\n",
    "        trainset_augmented, batch_size=4, shuffle=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    # net = Net()\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0\n",
    "        data_size = 0\n",
    "        for i, data in enumerate(trainloader_augmented, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels.to(torch.int64))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            data_size += inputs.shape[0]\n",
    "\n",
    "in_preds = []\n",
    "in_label = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        in_preds.append(outputs)\n",
    "        in_label.append(labels)\n",
    "    in_preds = torch.cat(in_preds)\n",
    "    in_label = torch.cat(in_label)\n",
    "print(\"----\")\n",
    "print(f\"{len(additional_x)} test cases generated\")\n",
    "print(\n",
    "    \"Test Accuracy (after) is: \",\n",
    "    accuracy_score(np.array(torch.argmax(in_preds, axis=1)), np.array(in_label)),\n",
    ")\n",
    "nc = NCT.coverage(trainloader_augmented)\n",
    "print(\"NC (after): \", nc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
