<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quick Start &mdash; AIJack v1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->

        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="aijack package" href="aijack.html" />
    <link rel="prev" title="Welcome to AIJack’s documentation!" href="index.html" />
</head>

<body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> AIJack
          </a>
              <div class="version">
                v1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick Start</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#install">Install</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-algorithms">Supported Algorithms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#collaborative-learning">1. Collaborative Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nn">1.1. NN</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tree">1.2. Tree</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#attack">2. Attack</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#model-inversion-attack">2.1. Model Inversion Attack</a></li>
<li class="toctree-l4"><a class="reference internal" href="#membership-inference-attack">2.2. Membership Inference Attack</a></li>
<li class="toctree-l4"><a class="reference internal" href="#label-leakage-attack">2.3. Label Leakage Attack</a></li>
<li class="toctree-l4"><a class="reference internal" href="#evasion-attack">2.4. Evasion Attack</a></li>
<li class="toctree-l4"><a class="reference internal" href="#poisoning-attack">2.5. Poisoning Attack</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#defense">3. Defense</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#differential-privacy">3.1. Differential Privacy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#homomorphic-encryption">3.2 Homomorphic Encryption</a></li>
<li class="toctree-l4"><a class="reference internal" href="#others">3.3. Others</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#resources">Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="#contact">Contact</a></li>
<li class="toctree-l2"><a class="reference internal" href="#examples-of-usage">Examples of Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#collaborative-learning-1">Collaborative Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#attack-against-federated-learning">Attack against Federated Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defense-for-federated-learning">Defense for Federated Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#attack-against-split-learning">Attack against Split Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-attacks">Other Attacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-defences">Other Defences</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="aijack.html">aijack package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AIJack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Quick Start</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/README.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">

  <!--
  Title: AIJack
  Description: AIJack is a fantastic framework to demonstrate security risks of machine learning and deep learning, such as model inversion attack, poisoning attack, and membership inference attack.
  Author: Hideaki Takahashi
  --><h1 align="center"><p>Try to hijack AI!</p>
</h1><section id="quick-start">
<h1>Quick Start<a class="headerlink" href="#quick-start" title="Permalink to this headline"></a></h1>
<p>This package implements algorithms for AI security such as Model
Inversion, Poisoning Attack, Evasion Attack, Differential Privacy, and
Homomorphic Encryption.</p>
<section id="install">
<h2>Install<a class="headerlink" href="#install" title="Permalink to this headline"></a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># pip install pybind11 (uncomment if necessary)</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">Koukyosyumei</span><span class="o">/</span><span class="n">AIJack</span>
</pre></div>
</div>
</section>
<section id="supported-algorithms">
<h2>Supported Algorithms<a class="headerlink" href="#supported-algorithms" title="Permalink to this headline"></a></h2>
<section id="collaborative-learning">
<h3>1. Collaborative Learning<a class="headerlink" href="#collaborative-learning" title="Permalink to this headline"></a></h3>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">globe_with_meridians</dt>
<dd class="field-odd"><p>Train a single model without sharing the</p>
</dd>
</dl>
<p>private datasets of multiple clients.</p>
</div></blockquote>
<section id="nn">
<h4>1.1. NN<a class="headerlink" href="#nn" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>FedAVG (<a class="reference external" href="example/model_inversion/soteria.py">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/1602.05629">paper</a>)</p></li>
<li><p>FedProx (<a class="reference external" href="https://arxiv.org/abs/1812.06127">paper</a>)</p></li>
<li><p>FedKD (<a class="reference external" href="test/collaborative/fedkd/test_fedkd.py">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/2108.13323">paper</a>)</p></li>
<li><p>FedMD (<a class="reference external" href="https://arxiv.org/abs/1910.03581">paper</a>)</p></li>
<li><p>FedGEMS (<a class="reference external" href="https://arxiv.org/abs/2110.11027">paper</a>)</p></li>
<li><p>DSFL (<a class="reference external" href="https://arxiv.org/abs/2008.06180">paper</a>)</p></li>
<li><p>SplitNN (<a class="reference external" href="example/label_leakage/label_leakage.py">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/1812.00564">paper</a>)</p></li>
</ul>
</section>
<section id="tree">
<h4>1.2. Tree<a class="headerlink" href="#tree" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>[WIP] SecureBoost
(<a class="reference external" href="test/collaborative/secureboost/test_secureboost.py">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/1901.08755">paper</a>)</p></li>
</ul>
</section>
</section>
<section id="attack">
<h3>2. Attack<a class="headerlink" href="#attack" title="Permalink to this headline"></a></h3>
<section id="model-inversion-attack">
<h4>2.1. Model Inversion Attack<a class="headerlink" href="#model-inversion-attack" title="Permalink to this headline"></a></h4>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">zap</dt>
<dd class="field-odd"><p>Reconstruct the private training dataset from the victim’s</p>
</dd>
</dl>
<p>model.</p>
</div></blockquote>
<ul class="simple">
<li><p>MI-FACE (<a class="reference external" href="example/model_inversion/mi_face.py">example</a>)
(<a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/2810103.2813677">paper</a>)</p></li>
<li><p>DLG
(<a class="reference external" href="example/model_inversion/gradient_inversion_attack.md">example</a>)
(<a class="reference external" href="https://papers.nips.cc/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html">paper</a>)</p></li>
<li><p>iDLG
(<a class="reference external" href="example/model_inversion/gradient_inversion_attack.md">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/2001.02610">paper</a>)</p></li>
<li><p>GS
(<a class="reference external" href="example/model_inversion/gradient_inversion_attack.md">example</a>)
(<a class="reference external" href="https://proceedings.neurips.cc/paper/2020/hash/c4ede56bbd98819ae6112b20ac6bf145-Abstract.html">paper</a>)</p></li>
<li><p>CPL
(<a class="reference external" href="example/model_inversion/gradient_inversion_attack.md">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/2004.10397">paper</a>)</p></li>
<li><p>GradInversion
(<a class="reference external" href="example/model_inversion/gradient_inversion_attack.md">example</a>)
(<a class="reference external" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yin_See_Through_Gradients_Image_Batch_Recovery_via_GradInversion_CVPR_2021_paper.pdf">paper</a>)</p></li>
<li><p>GAN attack (<a class="reference external" href="example/model_inversion/gan_attack.py">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/1702.07464">paper</a>)</p></li>
</ul>
</section>
<section id="membership-inference-attack">
<h4>2.2. Membership Inference Attack<a class="headerlink" href="#membership-inference-attack" title="Permalink to this headline"></a></h4>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">zap</dt>
<dd class="field-odd"><p>Determine whether the model’s training dataset contains the</p>
</dd>
</dl>
<p>target record.</p>
</div></blockquote>
<ul class="simple">
<li><p>Blak-box attack with shadow models
(<a class="reference external" href="example/membership_inference/membership_inference_CIFAR10.ipynb">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/1610.05820">paper</a>)</p></li>
</ul>
</section>
<section id="label-leakage-attack">
<h4>2.3. Label Leakage Attack<a class="headerlink" href="#label-leakage-attack" title="Permalink to this headline"></a></h4>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">zap</dt>
<dd class="field-odd"><p>Infer the label information of the dataset.</p>
</dd>
</dl>
</div></blockquote>
<ul class="simple">
<li><p>Norm attack (<a class="reference external" href="example/label_leakage/label_leakage.py">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/2102.08504">paper</a>)</p></li>
</ul>
</section>
<section id="evasion-attack">
<h4>2.4. Evasion Attack<a class="headerlink" href="#evasion-attack" title="Permalink to this headline"></a></h4>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">zap</dt>
<dd class="field-odd"><p>Generate data that the victim model cannot classify</p>
</dd>
</dl>
<p>correctly.</p>
</div></blockquote>
<ul class="simple">
<li><p>Gradient descent attacks
(<a class="reference external" href="example/adversarial_example/example_evasion_attack_svm.ipynb">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/1708.06131">paper</a>)</p></li>
</ul>
</section>
<section id="poisoning-attack">
<h4>2.5. Poisoning Attack<a class="headerlink" href="#poisoning-attack" title="Permalink to this headline"></a></h4>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">zap</dt>
<dd class="field-odd"><p>Inject malicious data into the training dataset to control</p>
</dd>
</dl>
<p>the behavior of the trained models.</p>
</div></blockquote>
<ul class="simple">
<li><p>Poisoning attack against support vector machines
(<a class="reference external" href="example/adversarial_example/example_poison_attack.ipynb">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/1206.6389">paper</a>)</p></li>
</ul>
</section>
</section>
<section id="defense">
<h3>3. Defense<a class="headerlink" href="#defense" title="Permalink to this headline"></a></h3>
<section id="differential-privacy">
<h4>3.1. Differential Privacy<a class="headerlink" href="#differential-privacy" title="Permalink to this headline"></a></h4>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">closed_lock_with_key</dt>
<dd class="field-odd"><p>Provide statistical privacy guarantee.</p>
</dd>
</dl>
</div></blockquote>
<ul class="simple">
<li><p>DPSGD
(<a class="reference external" href="example/model_inversion/mi_face_differential_privacy.py">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/1607.00133">paper</a>)</p></li>
</ul>
</section>
<section id="homomorphic-encryption">
<h4>3.2 Homomorphic Encryption<a class="headerlink" href="#homomorphic-encryption" title="Permalink to this headline"></a></h4>
<blockquote>
<div><dl class="field-list simple">
<dt class="field-odd">closed_lock_with_key</dt>
<dd class="field-odd"><p>Perform mathematical operations on</p>
</dd>
</dl>
<p>encrypted data</p>
</div></blockquote>
<ul class="simple">
<li><p>[WIP] CKKS (<a class="reference external" href="test/defense/ckks/test_core.py">example</a>)</p></li>
</ul>
</section>
<section id="others">
<h4>3.3. Others<a class="headerlink" href="#others" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>Soteria (<a class="reference external" href="example/model_inversion/soteria.py">example</a>)
(<a class="reference external" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Soteria_Provable_Defense_Against_Privacy_Leakage_in_Federated_Learning_From_CVPR_2021_paper.pdf">paper</a>)</p></li>
<li><p>MID (<a class="reference external" href="example/model_inversion/mid.ipynb">example</a>)
(<a class="reference external" href="https://arxiv.org/abs/2009.05241">paper</a>)</p></li>
</ul>
</section>
</section>
</section>
<section id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline"></a></h2>
<div class="line-block">
<div class="line">[WIP] <a class="reference external" href="https://koukyosyumei.github.io/AIJack">Official documentations
(https://koukyosyumei.github.io/AIJack)</a></div>
<div class="line">[WIP]
<a class="reference external" href="https://github.com/Koukyosyumei/AIJack/tree/main/example">Examples</a></div>
</div>
</section>
<section id="contact">
<h2>Contact<a class="headerlink" href="#contact" title="Permalink to this headline"></a></h2>
<p>welcome2aijack[&#64;]gmail.com</p>
</section>
<hr class="docutils" />
<section id="examples-of-usage">
<h2>Examples of Usage<a class="headerlink" href="#examples-of-usage" title="Permalink to this headline"></a></h2>
<section id="collaborative-learning-1">
<span id="id1"></span><h3>Collaborative Learning<a class="headerlink" href="#collaborative-learning-1" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>FedAVG</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">aijack.collaborative</span> <span class="kn">import</span> <span class="n">FedAvgClient</span><span class="p">,</span> <span class="n">FedAvgServer</span>

<span class="n">clients</span> <span class="o">=</span> <span class="p">[</span><span class="n">FedAvgClient</span><span class="p">(</span><span class="n">local_model_1</span><span class="p">,</span> <span class="n">user_id</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">FedAvgClient</span><span class="p">(</span><span class="n">local_model_2</span><span class="p">,</span> <span class="n">user_id</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">[</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">clients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">clients</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())]</span>
<span class="n">server</span> <span class="o">=</span> <span class="n">FedAvgServer</span><span class="p">(</span><span class="n">clients</span><span class="p">,</span> <span class="n">global_model</span><span class="p">)</span>

<span class="k">for</span> <span class="n">client</span><span class="p">,</span> <span class="n">local_trainloader</span><span class="p">,</span> <span class="n">local_optimizer</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">clients</span><span class="p">,</span> <span class="n">trainloaders</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">local_trainloader</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">local_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">client</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
        <span class="n">client</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="n">server</span><span class="o">.</span><span class="n">action</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>SplitNN</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">aijack.collaborative</span> <span class="kn">import</span> <span class="n">SplitNN</span><span class="p">,</span> <span class="n">SplitNNClient</span>

<span class="n">clients</span> <span class="o">=</span> <span class="p">[</span><span class="n">SplitNNClient</span><span class="p">(</span><span class="n">model_1</span><span class="p">,</span> <span class="n">user_id</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">SplitNNClient</span><span class="p">(</span><span class="n">model_2</span><span class="p">,</span> <span class="n">user_id</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">[</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_1</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_2</span><span class="o">.</span><span class="n">parameters</span><span class="p">())]</span>
<span class="n">splitnn</span> <span class="o">=</span> <span class="n">SplitNN</span><span class="p">(</span><span class="n">clients</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">splitnn</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">splitnn</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">splitnn</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">splitnn</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="attack-against-federated-learning">
<h3>Attack against Federated Learning<a class="headerlink" href="#attack-against-federated-learning" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">GradientInversion_Attack</span>

<span class="c1"># DLG Attack (Zhu, Ligeng, Zhijian Liu, and Song Han. &quot;Deep leakage from gradients.&quot; Advances in Neural Information Processing Systems 32 (2019).)</span>
<span class="n">dlg_manager</span> <span class="o">=</span> <span class="n">GradientInversionAttackManager</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">distancename</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">)</span>
<span class="n">FedAvgServer_DLG</span> <span class="o">=</span> <span class="n">dlg</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">FedAvgServer</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># GS Attack (Geiping, Jonas, et al. &quot;Inverting gradients-how easy is it to break privacy in federated learning?.&quot; Advances in Neural Information Processing Systems 33 (2020): 16937-16947.)</span>
<span class="sd">gs_manager = GradientInversionAttackManager(input_shape, distancename=&quot;cossim&quot;, tv_reg_coef=0.01)</span>
<span class="sd">FedAvgServer_GS = gs.attach(FedAvgServer)</span>

<span class="sd"># iDLG (Zhao, Bo, Konda Reddy Mopuri, and Hakan Bilen. &quot;idlg: Improved deep leakage from gradients.&quot; arXiv preprint arXiv:2001.02610 (2020).)</span>
<span class="sd">idlg_manager = GradientInversionAttackManager(input_shape, distancename=&quot;l2&quot;, optimize_label=False)</span>
<span class="sd">FedAvgServer_iDLG = idlg.attach(FedAvgServer)</span>

<span class="sd"># CPL (Wei, Wenqi, et al. &quot;A framework for evaluating gradient leakage attacks in federated learning.&quot; arXiv preprint arXiv:2004.10397 (2020).)</span>
<span class="sd">cpl_manager = GradientInversionAttackManager(input_shape, distancename=&quot;l2&quot;, optimize_label=False, lm_reg_coef=0.01)</span>
<span class="sd">FedAvgServer_CPL = cpl.attach(FedAvgServer)</span>

<span class="sd"># GradInversion (Yin, Hongxu, et al. &quot;See through gradients: Image batch recovery via gradinversion.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.)</span>
<span class="sd">gi_manager = GradientInversionAttackManager(input_shape, distancename=&quot;l2&quot;, optimize_label=False, bn_reg_layers=[net.body[1], net.body[4], net.body[7]],</span>
<span class="sd">                                    group_num = 5, tv_reg_coef=0.00, l2_reg_coef=0.0001, bn_reg_coef=0.001, gc_reg_coef=0.001)</span>
<span class="sd">FedAvgServer_GI = gi.attach(FedAvgServer)</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">server</span> <span class="o">=</span> <span class="n">FedAvgServer_DLG</span><span class="p">(</span><span class="n">clients</span><span class="p">,</span> <span class="n">global_model</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="c1"># --- normal federated learning --- #</span>
<span class="n">reconstructed_image</span><span class="p">,</span> <span class="n">reconstructed_label</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">attack</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>GAN Attack (client-side model inversion attack against federated
learning)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hitaj, Briland, Giuseppe Ateniese, and Fernando Perez-Cruz. &quot;Deep models under the GAN: information leakage from collaborative deep learning.&quot; Proceedings of the # 2017 ACM SIGSAC Conference on Computer and Communications Security. 2017.</span>
<span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">GANAttackManager</span>
<span class="kn">from</span> <span class="nn">aijack.collaborative</span> <span class="kn">import</span> <span class="n">FedAvgClient</span>

<span class="n">manager</span> <span class="o">=</span> <span class="n">GANAttackManager</span><span class="p">(</span>
    <span class="n">target_label</span><span class="p">,</span>
    <span class="n">generator</span><span class="p">,</span>
    <span class="n">optimizer_g</span><span class="p">,</span>
    <span class="n">criterion</span><span class="p">,</span>
    <span class="n">nz</span><span class="o">=</span><span class="n">nz</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">GANAttackFedAvgClient</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">FedAvgClient</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">GANAttackFedAvgClient</span><span class="p">(</span><span class="n">client</span><span class="p">)</span>
<span class="c1"># --- normal federated learning --- #</span>
<span class="n">reconstructed_image</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">attack</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="defense-for-federated-learning">
<h3>Defense for Federated Learning<a class="headerlink" href="#defense-for-federated-learning" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Soteria</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sun, Jingwei, et al. &quot;Soteria: Provable defense against privacy leakage in federated learning from representation perspective.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.</span>
<span class="kn">from</span> <span class="nn">aijack.collaborative</span> <span class="kn">import</span> <span class="n">FedAvgClient</span>
<span class="kn">from</span> <span class="nn">aijack.defense</span> <span class="kn">import</span> <span class="n">SoteriaManager</span>

<span class="n">manager</span> <span class="o">=</span> <span class="n">SoteriaManager</span><span class="p">(</span><span class="s2">&quot;conv&quot;</span><span class="p">,</span> <span class="s2">&quot;lin&quot;</span><span class="p">,</span> <span class="n">target_layer_name</span><span class="o">=</span><span class="s2">&quot;lin.0.weight&quot;</span><span class="p">)</span>
<span class="n">SoteriaFedAvgClient</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">FedAvgClient</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">SoteriaFedAvgClient</span><span class="p">(</span><span class="n">Net</span><span class="p">(),</span> <span class="n">user_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="c1"># --- normal FL training ---</span>
</pre></div>
</div>
</section>
<section id="attack-against-split-learning">
<h3>Attack against Split Learning<a class="headerlink" href="#attack-against-split-learning" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Label Leakage Attack</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Li, Oscar, et al. &quot;Label leakage and protection in two-party split learning.&quot; arXiv preprint arXiv:2102.08504 (2021).</span>
<span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">NormAttackManager</span>
<span class="kn">from</span> <span class="nn">aijack.collaborative</span> <span class="kn">import</span> <span class="n">SplitNN</span>

<span class="n">manager</span> <span class="o">=</span> <span class="n">NormAttackManager</span><span class="p">(</span><span class="n">criterion</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">NormAttackSplitNN</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">SplitNN</span><span class="p">)</span>
<span class="n">normattacksplitnn</span> <span class="o">=</span> <span class="n">NormAttackSplitNN</span><span class="p">(</span><span class="n">clients</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">)</span>
<span class="c1"># --- normal split learning --- #</span>
<span class="n">leak_auc</span> <span class="o">=</span> <span class="n">normattacksplitnn</span><span class="o">.</span><span class="n">attack</span><span class="p">(</span><span class="n">target_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="other-attacks">
<h3>Other Attacks<a class="headerlink" href="#other-attacks" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>MI-FACE (model inversion attack)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fredrikson, Matt, Somesh Jha, and Thomas Ristenpart. &quot;Model inversion attacks that exploit confidence information and basic countermeasures.&quot; Proceedings of the 22nd # ACM SIGSAC conference on computer and communications security. 2015.</span>
<span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">MI_FACE</span>

<span class="n">mi</span> <span class="o">=</span> <span class="n">MI_FACE</span><span class="p">(</span><span class="n">target_torch_net</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span>
<span class="n">reconstructed_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">mi</span><span class="o">.</span><span class="n">attack</span><span class="p">(</span><span class="n">target_label</span><span class="p">,</span> <span class="n">lam</span><span class="p">,</span> <span class="n">num_itr</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Evasion Attack</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Biggio, Battista, et al. &quot;Evasion attacks against machine learning at test time.&quot; Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2013.</span>
<span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">Evasion_attack_sklearn</span>

<span class="n">attacker</span> <span class="o">=</span> <span class="n">Evasion_attack_sklearn</span><span class="p">(</span><span class="n">target_model</span><span class="o">=</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_minus_1</span><span class="o">=</span><span class="n">attackers_dataset</span><span class="p">)</span>
<span class="n">result</span><span class="p">,</span> <span class="n">log</span> <span class="o">=</span> <span class="n">attacker</span><span class="o">.</span><span class="n">attack</span><span class="p">(</span><span class="n">initial_datapoint</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Poisoning Attack</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Biggio, Battista, Blaine Nelson, and Pavel Laskov. &quot;Poisoning attacks against support vector machines.&quot; arXiv preprint arXiv:1206.6389 (2012).</span>
<span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">Poison_attack_sklearn</span>

<span class="n">attacker</span> <span class="o">=</span> <span class="n">Poison_attack_sklearn</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train_</span><span class="p">,</span> <span class="n">y_train_</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">xc_attacked</span><span class="p">,</span> <span class="n">log</span> <span class="o">=</span> <span class="n">attacker</span><span class="o">.</span><span class="n">attack</span><span class="p">(</span><span class="n">xc</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="other-defences">
<h3>Other Defences<a class="headerlink" href="#other-defences" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>DPSGD (Differential Privacy)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#  Abadi, Martin, et al. &quot;Deep learning with differential privacy.&quot; Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.</span>
<span class="kn">from</span> <span class="nn">aijack.defense</span> <span class="kn">import</span> <span class="n">GeneralMomentAccountant</span>
<span class="kn">from</span> <span class="nn">aijack.defense</span> <span class="kn">import</span> <span class="n">PrivacyManager</span>

<span class="n">accountant</span> <span class="o">=</span> <span class="n">GeneralMomentAccountant</span><span class="p">(</span><span class="n">noise_type</span><span class="o">=</span><span class="s2">&quot;Gaussian&quot;</span><span class="p">,</span> <span class="n">search</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">,</span> <span class="n">orders</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">bound_type</span><span class="o">=</span><span class="s2">&quot;rdp_tight_upperbound&quot;</span><span class="p">)</span>
<span class="n">privacy_manager</span> <span class="o">=</span> <span class="n">PrivacyManager</span><span class="p">(</span><span class="n">accountant</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">,</span> <span class="n">l2_norm_clip</span><span class="o">=</span><span class="n">l2_norm_clip</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">trainset</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="n">iterations</span><span class="p">)</span>
<span class="n">dpoptimizer_cls</span><span class="p">,</span> <span class="n">lot_loader</span><span class="p">,</span> <span class="n">batch_loader</span> <span class="o">=</span> <span class="n">privacy_manager</span><span class="o">.</span><span class="n">privatize</span><span class="p">(</span><span class="n">noise_multiplier</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">lot_loader</span><span class="p">(</span><span class="n">trainset</span><span class="p">):</span>
    <span class="n">X_lot</span><span class="p">,</span> <span class="n">y_lot</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">batch_loader</span><span class="p">(</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_lot</span><span class="p">,</span> <span class="n">y_lot</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad_keep_accum_grads</span><span class="p">()</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X_batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">update_accum_grads</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>MID</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Wang, Tianhao, Yuheng Zhang, and Ruoxi Jia. &quot;Improving robustness to model inversion attacks via mutual information regularization.&quot; arXiv preprint arXiv:2009.05241 (2020).</span>
<span class="kn">from</span> <span class="nn">aijack.defense</span> <span class="kn">import</span> <span class="n">VIB</span><span class="p">,</span> <span class="n">mib_loss</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">VIB</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">dim_of_latent_space</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">samples_amount</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">y_pred</span><span class="p">,</span> <span class="n">result_dict</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x_batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">y_batch</span><span class="p">,</span> <span class="n">result_dict</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to AIJack’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="aijack.html" class="btn btn-neutral float-right" title="aijack package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Hideaki Takahashi.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

</body>
</html>
