<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quick Start &mdash; AIJack v1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->

        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="aijack package" href="aijack.html" />
    <link rel="prev" title="Welcome to AIJack’s documentation!" href="index.html" />
</head>

<body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> AIJack
          </a>
              <div class="version">
                v1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick Start</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#install">Install</a></li>
<li class="toctree-l2"><a class="reference internal" href="#usage">Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#collaborative-learning">Collaborative Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#attack-against-federated-learning">Attack against Federated Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defense-for-federated-learning">Defense for Federated Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#attack-against-split-learning">Attack against Split Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-attacks">Other Attacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-defences">Other Defences</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="aijack.html">aijack package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AIJack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Quick Start</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/README.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">

  <!--
  Title: AIJack
  Description: AIJack is a fantastic framework to demonstrate security risks of machine learning and deep learning, such as model inversion attack, poisoning attack, and membership inference attack.
  Author: Hideaki Takahashi
  --><h1 align="center"><p>Try to hijack AI!</p>
</h1><section id="quick-start">
<h1>Quick Start<a class="headerlink" href="#quick-start" title="Permalink to this headline"></a></h1>
<p>This package implements algorithms for AI security such as Model
Inversion, Poisoning Attack, Evasion Attack, Differential Privacy, and
Homomorphic Encryption.</p>
<div class="line-block">
<div class="line"><a class="reference external" href="https://koukyosyumei.github.io/AIJack">Official documentations</a></div>
<div class="line"><a class="reference external" href="https://github.com/Koukyosyumei/AIJack/tree/main/example">Examples</a></div>
</div>
<section id="install">
<h2>Install<a class="headerlink" href="#install" title="Permalink to this headline"></a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># pip install pybind11 (uncomment if necessary)</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">Koukyosyumei</span><span class="o">/</span><span class="n">AIJack</span>
</pre></div>
</div>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this headline"></a></h2>
<section id="collaborative-learning">
<h3>Collaborative Learning<a class="headerlink" href="#collaborative-learning" title="Permalink to this headline"></a></h3>
<p>AIJack allows you to simulate the collaborative learning, where multiple
clients trains a single model without sharing their private datasets.</p>
<ul class="simple">
<li><p>Federated Learning</p></li>
</ul>
<p>In Federated Learning, the clients communicates their locally trained
models and the server aggregates the received local models and creates a
global model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">aijack.collaborative</span> <span class="kn">import</span> <span class="n">FedAvgClient</span><span class="p">,</span> <span class="n">FedAvgServer</span>

<span class="n">clients</span> <span class="o">=</span> <span class="p">[</span><span class="n">FedAvgClient</span><span class="p">(</span><span class="n">local_model_1</span><span class="p">,</span> <span class="n">user_id</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">FedAvgClient</span><span class="p">(</span><span class="n">local_model_2</span><span class="p">,</span> <span class="n">user_id</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">[</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">clients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">clients</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())]</span>
<span class="n">server</span> <span class="o">=</span> <span class="n">FedAvgServer</span><span class="p">(</span><span class="n">clients</span><span class="p">,</span> <span class="n">global_model</span><span class="p">)</span>

<span class="k">for</span> <span class="n">client</span><span class="p">,</span> <span class="n">local_trainloader</span><span class="p">,</span> <span class="n">local_optimizer</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">clients</span><span class="p">,</span> <span class="n">trainloaders</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">local_trainloader</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">local_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">client</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
        <span class="n">client</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="n">server</span><span class="o">.</span><span class="n">action</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Split Learning</p></li>
</ul>
<p>In Split Learning, only one client has the label of the training
dataset, and each client communicates the gradient of the intermidiate
layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">aijack.collaborative</span> <span class="kn">import</span> <span class="n">SplitNN</span><span class="p">,</span> <span class="n">SplitNNClient</span>

<span class="n">clients</span> <span class="o">=</span> <span class="p">[</span><span class="n">SplitNNClient</span><span class="p">(</span><span class="n">model_1</span><span class="p">,</span> <span class="n">user_id</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">SplitNNClient</span><span class="p">(</span><span class="n">model_2</span><span class="p">,</span> <span class="n">user_id</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">[</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_1</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model_2</span><span class="o">.</span><span class="n">parameters</span><span class="p">())]</span>
<span class="n">splitnn</span> <span class="o">=</span> <span class="n">SplitNN</span><span class="p">(</span><span class="n">clients</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">splitnn</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">splitnn</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">splitnn</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">splitnn</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="attack-against-federated-learning">
<h3>Attack against Federated Learning<a class="headerlink" href="#attack-against-federated-learning" title="Permalink to this headline"></a></h3>
<p>AIJack implements several attack algorithms to violate the safety of
Federated Learning. You can attach the attack ability to the server or
client classes with <code class="docutils literal notranslate"><span class="pre">attach</span></code> methods of <code class="docutils literal notranslate"><span class="pre">AttackManager</span></code> of each
attack algorithms.</p>
<ul class="simple">
<li><p>Gradient Inversion (server-side model inversion attack against
federated learning)</p></li>
</ul>
<p>Gradients communicated between the server and the clients might leak
private information, and the malicious server might be able to
reconstruct the training data from the gradients.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">GradientInversion_Attack</span>

<span class="c1"># DLG Attack (Zhu, Ligeng, Zhijian Liu, and Song Han. &quot;Deep leakage from gradients.&quot; Advances in Neural Information Processing Systems 32 (2019).)</span>
<span class="n">dlg_manager</span> <span class="o">=</span> <span class="n">GradientInversionAttackManager</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">distancename</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">)</span>
<span class="n">FedAvgServer_DLG</span> <span class="o">=</span> <span class="n">dlg</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">FedAvgServer</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd"># GS Attack (Geiping, Jonas, et al. &quot;Inverting gradients-how easy is it to break privacy in federated learning?.&quot; Advances in Neural Information Processing Systems 33 (2020): 16937-16947.)</span>
<span class="sd">gs_manager = GradientInversionAttackManager(input_shape, distancename=&quot;cossim&quot;, tv_reg_coef=0.01)</span>
<span class="sd">FedAvgServer_GS = gs.attach(FedAvgServer)</span>

<span class="sd"># iDLG (Zhao, Bo, Konda Reddy Mopuri, and Hakan Bilen. &quot;idlg: Improved deep leakage from gradients.&quot; arXiv preprint arXiv:2001.02610 (2020).)</span>
<span class="sd">idlg_manager = GradientInversionAttackManager(input_shape, distancename=&quot;l2&quot;, optimize_label=False)</span>
<span class="sd">FedAvgServer_iDLG = idlg.attach(FedAvgServer)</span>

<span class="sd"># CPL (Wei, Wenqi, et al. &quot;A framework for evaluating gradient leakage attacks in federated learning.&quot; arXiv preprint arXiv:2004.10397 (2020).)</span>
<span class="sd">cpl_manager = GradientInversionAttackManager(input_shape, distancename=&quot;l2&quot;, optimize_label=False, lm_reg_coef=0.01)</span>
<span class="sd">FedAvgServer_CPL = cpl.attach(FedAvgServer)</span>

<span class="sd"># GradInversion (Yin, Hongxu, et al. &quot;See through gradients: Image batch recovery via gradinversion.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.)</span>
<span class="sd">gi_manager = GradientInversionAttackManager(input_shape, distancename=&quot;l2&quot;, optimize_label=False, bn_reg_layers=[net.body[1], net.body[4], net.body[7]],</span>
<span class="sd">                                    group_num = 5, tv_reg_coef=0.00, l2_reg_coef=0.0001, bn_reg_coef=0.001, gc_reg_coef=0.001)</span>
<span class="sd">FedAvgServer_GI = gi.attach(FedAvgServer)</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="n">server</span> <span class="o">=</span> <span class="n">FedAvgServer_DLG</span><span class="p">(</span><span class="n">clients</span><span class="p">,</span> <span class="n">global_model</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="c1"># --- normal federated learning --- #</span>
<span class="n">reconstructed_image</span><span class="p">,</span> <span class="n">reconstructed_label</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">attack</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>GAN Attack (client-side model inversion attack against federated
learning)</p></li>
</ul>
<p>The malicious client might be able to reconstruct the private training
data of other clients with GAN Attack.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hitaj, Briland, Giuseppe Ateniese, and Fernando Perez-Cruz. &quot;Deep models under the GAN: information leakage from collaborative deep learning.&quot; Proceedings of the # 2017 ACM SIGSAC Conference on Computer and Communications Security. 2017.</span>
<span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">GANAttackManager</span>
<span class="kn">from</span> <span class="nn">aijack.collaborative</span> <span class="kn">import</span> <span class="n">FedAvgClient</span>

<span class="n">manager</span> <span class="o">=</span> <span class="n">GANAttackManager</span><span class="p">(</span>
    <span class="n">target_label</span><span class="p">,</span>
    <span class="n">generator</span><span class="p">,</span>
    <span class="n">optimizer_g</span><span class="p">,</span>
    <span class="n">criterion</span><span class="p">,</span>
    <span class="n">nz</span><span class="o">=</span><span class="n">nz</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">GANAttackFedAvgClient</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">FedAvgClient</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">GANAttackFedAvgClient</span><span class="p">(</span><span class="n">client</span><span class="p">)</span>
<span class="c1"># --- normal federated learning --- #</span>
<span class="n">reconstructed_image</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">attack</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="defense-for-federated-learning">
<h3>Defense for Federated Learning<a class="headerlink" href="#defense-for-federated-learning" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Soteria</p></li>
</ul>
<p>Soteria prevents information leakage from the gradients by adding noise
to the gradients. You can apply Soteria to any clients with <code class="docutils literal notranslate"><span class="pre">attach</span></code>
method of <code class="docutils literal notranslate"><span class="pre">SoteriaManager</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sun, Jingwei, et al. &quot;Soteria: Provable defense against privacy leakage in federated learning from representation perspective.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.</span>
<span class="kn">from</span> <span class="nn">aijack.collaborative</span> <span class="kn">import</span> <span class="n">FedAvgClient</span>
<span class="kn">from</span> <span class="nn">aijack.defense</span> <span class="kn">import</span> <span class="n">SoteriaManager</span>

<span class="n">manager</span> <span class="o">=</span> <span class="n">SoteriaManager</span><span class="p">(</span><span class="s2">&quot;conv&quot;</span><span class="p">,</span> <span class="s2">&quot;lin&quot;</span><span class="p">,</span> <span class="n">target_layer_name</span><span class="o">=</span><span class="s2">&quot;lin.0.weight&quot;</span><span class="p">)</span>
<span class="n">SoteriaFedAvgClient</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">FedAvgClient</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">SoteriaFedAvgClient</span><span class="p">(</span><span class="n">Net</span><span class="p">(),</span> <span class="n">user_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="c1"># --- normal FL training ---</span>
</pre></div>
</div>
</section>
<section id="attack-against-split-learning">
<h3>Attack against Split Learning<a class="headerlink" href="#attack-against-split-learning" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Label Leakage Attack</p></li>
</ul>
<p>Label leakage attack is one of the known vulnerabilities of SplitNN, and
AIJack currently supports the norm-based label leakage attack.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Li, Oscar, et al. &quot;Label leakage and protection in two-party split learning.&quot; arXiv preprint arXiv:2102.08504 (2021).</span>
<span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">NormAttackManager</span>
<span class="kn">from</span> <span class="nn">aijack.collaborative</span> <span class="kn">import</span> <span class="n">SplitNN</span>

<span class="n">manager</span> <span class="o">=</span> <span class="n">NormAttackManager</span><span class="p">(</span><span class="n">criterion</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">NormAttackSplitNN</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">SplitNN</span><span class="p">)</span>
<span class="n">normattacksplitnn</span> <span class="o">=</span> <span class="n">NormAttackSplitNN</span><span class="p">(</span><span class="n">clients</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">)</span>
<span class="c1"># --- normal split learning --- #</span>
<span class="n">leak_auc</span> <span class="o">=</span> <span class="n">normattacksplitnn</span><span class="o">.</span><span class="n">attack</span><span class="p">(</span><span class="n">target_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="other-attacks">
<h3>Other Attacks<a class="headerlink" href="#other-attacks" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>MI-FACE (model inversion attack)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fredrikson, Matt, Somesh Jha, and Thomas Ristenpart. &quot;Model inversion attacks that exploit confidence information and basic countermeasures.&quot; Proceedings of the 22nd # ACM SIGSAC conference on computer and communications security. 2015.</span>
<span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">MI_FACE</span>

<span class="n">mi</span> <span class="o">=</span> <span class="n">MI_FACE</span><span class="p">(</span><span class="n">target_torch_net</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span>
<span class="n">reconstructed_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">mi</span><span class="o">.</span><span class="n">attack</span><span class="p">(</span><span class="n">target_label</span><span class="p">,</span> <span class="n">lam</span><span class="p">,</span> <span class="n">num_itr</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Evasion Attack</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Biggio, Battista, et al. &quot;Evasion attacks against machine learning at test time.&quot; Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2013.</span>
<span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">Evasion_attack_sklearn</span>

<span class="n">attacker</span> <span class="o">=</span> <span class="n">Evasion_attack_sklearn</span><span class="p">(</span><span class="n">target_model</span><span class="o">=</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_minus_1</span><span class="o">=</span><span class="n">attackers_dataset</span><span class="p">)</span>
<span class="n">result</span><span class="p">,</span> <span class="n">log</span> <span class="o">=</span> <span class="n">attacker</span><span class="o">.</span><span class="n">attack</span><span class="p">(</span><span class="n">initial_datapoint</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Poisoning Attack</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Biggio, Battista, Blaine Nelson, and Pavel Laskov. &quot;Poisoning attacks against support vector machines.&quot; arXiv preprint arXiv:1206.6389 (2012).</span>
<span class="kn">from</span> <span class="nn">aijack.attack</span> <span class="kn">import</span> <span class="n">Poison_attack_sklearn</span>

<span class="n">attacker</span> <span class="o">=</span> <span class="n">Poison_attack_sklearn</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train_</span><span class="p">,</span> <span class="n">y_train_</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">xc_attacked</span><span class="p">,</span> <span class="n">log</span> <span class="o">=</span> <span class="n">attacker</span><span class="o">.</span><span class="n">attack</span><span class="p">(</span><span class="n">xc</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="other-defences">
<h3>Other Defences<a class="headerlink" href="#other-defences" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>DPSGD (Differential Privacy)</p></li>
</ul>
<p>AIJack natively supports differential privacy, including moment
accountant and DPSGD.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#  Abadi, Martin, et al. &quot;Deep learning with differential privacy.&quot; Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.</span>
<span class="kn">from</span> <span class="nn">aijack.defense</span> <span class="kn">import</span> <span class="n">GeneralMomentAccountant</span>
<span class="kn">from</span> <span class="nn">aijack.defense</span> <span class="kn">import</span> <span class="n">PrivacyManager</span>

<span class="n">accountant</span> <span class="o">=</span> <span class="n">GeneralMomentAccountant</span><span class="p">(</span><span class="n">noise_type</span><span class="o">=</span><span class="s2">&quot;Gaussian&quot;</span><span class="p">,</span> <span class="n">search</span><span class="o">=</span><span class="s2">&quot;greedy&quot;</span><span class="p">,</span> <span class="n">orders</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">bound_type</span><span class="o">=</span><span class="s2">&quot;rdp_tight_upperbound&quot;</span><span class="p">)</span>
<span class="n">privacy_manager</span> <span class="o">=</span> <span class="n">PrivacyManager</span><span class="p">(</span><span class="n">accountant</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">,</span> <span class="n">l2_norm_clip</span><span class="o">=</span><span class="n">l2_norm_clip</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">trainset</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="n">iterations</span><span class="p">)</span>
<span class="n">dpoptimizer_cls</span><span class="p">,</span> <span class="n">lot_loader</span><span class="p">,</span> <span class="n">batch_loader</span> <span class="o">=</span> <span class="n">privacy_manager</span><span class="o">.</span><span class="n">privatize</span><span class="p">(</span><span class="n">noise_multiplier</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">lot_loader</span><span class="p">(</span><span class="n">trainset</span><span class="p">):</span>
    <span class="n">X_lot</span><span class="p">,</span> <span class="n">y_lot</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">batch_loader</span><span class="p">(</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_lot</span><span class="p">,</span> <span class="n">y_lot</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad_keep_accum_grads</span><span class="p">()</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X_batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">update_accum_grads</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>MID (Defense against model inversion attak)</p></li>
</ul>
<p>MID is a defense technique to prevent information leakage from the
output logits, and it minimizes the mutual information between output
logits and input data. AIJack currently supports VIB, a neural network
with MID. If you want to apply VIB to your network, you should split
your model into a first-half part (<code class="docutils literal notranslate"><span class="pre">encoder</span></code>) and a second-half part
(<code class="docutils literal notranslate"><span class="pre">decoder</span></code>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Wang, Tianhao, Yuheng Zhang, and Ruoxi Jia. &quot;Improving robustness to model inversion attacks via mutual information regularization.&quot; arXiv preprint arXiv:2009.05241 (2020).</span>
<span class="kn">from</span> <span class="nn">aijack.defense</span> <span class="kn">import</span> <span class="n">VIB</span><span class="p">,</span> <span class="n">mib_loss</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">VIB</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">dim_of_latent_space</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">samples_amount</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">y_pred</span><span class="p">,</span> <span class="n">result_dict</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x_batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">y_batch</span><span class="p">,</span> <span class="n">result_dict</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to AIJack’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="aijack.html" class="btn btn-neutral float-right" title="aijack package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Hideaki Takahashi.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

</body>
</html>
