{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-heavy",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "we'll use mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.fetch_openml('mnist_784', version=1, data_home=\".\", return_X_y=True)\n",
    "imagedata, labeldata = mnist[0],mnist[1]\n",
    "print(\"画像データ数:\"+str(imagedata.shape))\n",
    "print(\"ラベルデータ数:\"+str(labeldata.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = mnist[0].values / 255\n",
    "mnist_label = mnist[1].values\n",
    "\n",
    "print(mnist_data.shape)\n",
    "print(mnist_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-grammar",
   "metadata": {},
   "source": [
    "We devide our data three: train data, shadow data, and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "idxs = list(range(mnist_data.shape[0]))\n",
    "random.shuffle(idxs)\n",
    "\n",
    "train_idx = idxs[:500]\n",
    "shadow_idx = idxs[500:1000]\n",
    "eval_idx = idxs[1000:1500]\n",
    "\n",
    "train_data = mnist_data[train_idx]\n",
    "shadow_data = mnist_data[shadow_idx]\n",
    "eval_data = mnist_data[eval_idx]\n",
    "\n",
    "train_label = mnist_label[train_idx]\n",
    "shadow_label = mnist_label[shadow_idx]\n",
    "eval_label = mnist_label[eval_idx]\n",
    "\n",
    "print(\"train_data shape is \", train_data.shape)\n",
    "print(\"shadow_data shape is \", shadow_data.shape)\n",
    "print(\"eval_data shape is \", eval_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-assist",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Target model\n",
    "\n",
    "We assume that the target use SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = SVC(probability=True)\n",
    "target_model.fit(train_data, train_label)\n",
    "\n",
    "target_pred = target_model.predict(train_data)\n",
    "target_prob = target_model.predict_proba(train_data)\n",
    "\n",
    "eval_pred = target_model.predict(eval_data)\n",
    "eval_prob = target_model.predict_proba(eval_data)\n",
    "\n",
    "ac_score = metrics.accuracy_score(target_pred, train_label)\n",
    "print(ac_score)\n",
    "ac_score = metrics.accuracy_score(eval_pred, eval_label)\n",
    "print(ac_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-marble",
   "metadata": {},
   "source": [
    "# Shadow model\n",
    "\n",
    "We also use SVM as shadow model and create 5 models with k-fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5,\n",
    "           random_state=42,\n",
    "           shuffle=True)\n",
    "\n",
    "in_probs = []\n",
    "out_probs = []\n",
    "shadow_in_labels = []\n",
    "shadow_out_labels = []\n",
    "\n",
    "for trn_idx, val_idx in kf.split(shadow_data):\n",
    "    in_data = shadow_data[trn_idx]\n",
    "    out_data =shadow_data[val_idx]\n",
    "    in_label = shadow_label[trn_idx]\n",
    "    out_label = shadow_label[val_idx]\n",
    "    \n",
    "    shadow_model = SVC(probability=True)\n",
    "    shadow_model.fit(in_data, in_label)\n",
    "    \n",
    "    in_prob = shadow_model.predict_proba(in_data)\n",
    "    out_prob = shadow_model.predict_proba(out_data)\n",
    "    \n",
    "    in_probs.append(in_prob)\n",
    "    out_probs.append(out_prob)\n",
    "    \n",
    "    shadow_in_labels.append(in_label)\n",
    "    shadow_out_labels.append(out_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-statistics",
   "metadata": {},
   "source": [
    "create labels to train attack model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_probs = np.concatenate(in_probs)\n",
    "out_probs = np.concatenate(out_probs)\n",
    "\n",
    "in_labels = np.ones(in_probs.shape[0])\n",
    "out_labels = np.zeros(out_probs.shape[0])\n",
    "\n",
    "attack_data = np.concatenate([in_probs, out_probs])\n",
    "attack_label = np.concatenate([in_labels, out_labels])\n",
    "\n",
    "shadow_in_labels = np.concatenate(shadow_in_labels)\n",
    "shadow_out_labels = np.concatenate(shadow_out_labels)\n",
    "shadow_original_label = np.concatenate([shadow_in_labels,\n",
    "                                        shadow_out_labels])\n",
    "\n",
    "attack_data_idx = list(range(attack_data.shape[0]))\n",
    "random.shuffle(attack_data_idx)\n",
    "\n",
    "attack_data = attack_data[attack_data_idx]\n",
    "attack_label = attack_label[attack_data_idx]\n",
    "shadow_original_label = shadow_original_label[attack_data_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-flesh",
   "metadata": {},
   "source": [
    "# Attack model\n",
    "\n",
    "We make SVM classifier for each label as attack model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(shadow_original_label)\n",
    "\n",
    "all_attack_true_label = np.zeros_like(shadow_original_label).astype(int)\n",
    "all_attack_preds = np.zeros_like(shadow_original_label).astype(int)\n",
    "\n",
    "attack_model_dict = {ul:None for ul in unique_labels}\n",
    "\n",
    "for label in unique_labels:\n",
    "\n",
    "    label_idx = np.where(shadow_original_label == label)[0]\n",
    "\n",
    "    attack_label_data = attack_data[label_idx]\n",
    "    attack_label_label = attack_label[label_idx]\n",
    "\n",
    "    attack_model = SVC(probability=True)\n",
    "    attack_model.fit(attack_label_data, attack_label_label)\n",
    "    attack_pred = attack_model.predict(attack_label_data)\n",
    "\n",
    "    all_attack_true_label[label_idx] = attack_label_label\n",
    "    all_attack_preds[label_idx] = attack_pred\n",
    "    \n",
    "    attack_model_dict[label] = attack_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(all_attack_preds, all_attack_true_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-bailey",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_in = np.ones_like(train_label).astype(int)\n",
    "target_label_out = np.zeros_like(eval_label).astype(int)\n",
    "\n",
    "probs = np.concatenate([target_prob, eval_prob])\n",
    "label_in_out = np.concatenate([target_label_in, target_label_out])\n",
    "true_label = np.concatenate([train_label, eval_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_label_pred = np.zeros_like(label_in_out).astype(int)\n",
    "\n",
    "for label, label_model in attack_model_dict.items():\n",
    "    label_idx = np.where(true_label == label)[0]\n",
    "    \n",
    "    predict_in_out_label = attack_model_dict[label].predict(probs[label_idx])\n",
    "    true_in_out_label = label_in_out[label_idx]\n",
    "    \n",
    "    in_out_label_pred[label_idx] = predict_in_out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"overall f1 score is \", metrics.f1_score(in_out_label_pred, label_in_out))\n",
    "print(metrics.classification_report(in_out_label_pred, label_in_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-market",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
