{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from secure_ml.attack import Model_inversion\n",
    "from secure_ml.utils import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT PATHS:\n",
    "BASE = 'data/'\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "for i in range(1, 41):\n",
    "    for j in range(1, 11):\n",
    "        img = cv2.imread(BASE + f's{i}/{j}.pgm', 0)\n",
    "        imgs.append(img)\n",
    "        labels.append(i-1)\n",
    "        \n",
    "X = np.stack(imgs)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToTensor：画像のグレースケール化（RGBの0~255を0~1の範囲に正規化）、Normalize：Z値化（RGBの平均と標準偏差を0.5で決め打ちして正規化）\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "trainset = DataSet(X, y, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fla = nn.Flatten()\n",
    "        self.fc = nn.Linear(112*92, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fla(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.to(torch.int64))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # print statistics\n",
    "        #train_loss = loss.item()\n",
    "        #print('[%d, %5d] loss: %.3f' %\n",
    "        #          (epoch + 1, i + 1, train_loss))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "in_preds = []\n",
    "in_label = []\n",
    "with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            inputs, labels = data\n",
    "            outputs = net(inputs)\n",
    "            in_preds.append(outputs)\n",
    "            in_label.append(labels)  \n",
    "        in_preds = torch.cat(in_preds)\n",
    "        in_label = torch.cat(in_label)  \n",
    "print(accuracy_score(np.array(torch.argmax(in_preds, axis=1)),\n",
    "                     np.array(in_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 1, 112, 92)\n",
    "target_label_1 = 1\n",
    "target_label_2 = 10\n",
    "lam = 0.1\n",
    "num_itr = 100\n",
    "mi = Model_inversion(net, input_shape)\n",
    "x_result_1, log = mi.attack(target_label_1, lam, num_itr)\n",
    "x_result_2, log = mi.attack(target_label_2, lam, num_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(4, 5))\n",
    "axes[0][0].imshow(cv2.imread(BASE + f's2/1.pgm', 0), cmap='gray')\n",
    "axes[0][0].axis('off')\n",
    "axes[0][0].set_title('original image')\n",
    "axes[0][1].imshow(x_result_1[0][0], cmap='gray')\n",
    "axes[0][1].axis('off')\n",
    "axes[0][1].set_title('extracted image')\n",
    "\n",
    "axes[1][0].imshow(cv2.imread(BASE + f's11/1.pgm', 0), cmap='gray')\n",
    "axes[1][0].axis('off')\n",
    "axes[1][0].set_title('original image')\n",
    "axes[1][1].imshow(x_result_2[0][0], cmap='gray')\n",
    "axes[1][1].axis('off')\n",
    "axes[1][1].set_title('extracted image')\n",
    "# plt.savefig(\"model_inversion.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
